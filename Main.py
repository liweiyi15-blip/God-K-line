import discord
from discord import app_commands
from discord.ext import tasks
import json
import os
from datetime import datetime, time, timedelta
import asyncio
import pandas as pd
import numpy as np
import pytz
from dotenv import load_dotenv
from collections import defaultdict
import aiohttp
import io
import matplotlib
import random
import warnings

# [æ—¥å¿—é…ç½®]
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')

# --- å¼ºåˆ¶ä½¿ç”¨éäº¤äº’å¼åç«¯ ---
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import mplfinance as mpf

# --- åŠ è½½ç¯å¢ƒå˜é‡ ---
load_dotenv()

DISCORD_TOKEN = os.getenv("DISCORD_TOKEN")
FMP_API_KEY = os.getenv("FMP_API_KEY")

try:
    ALERT_CHANNEL_ID = int(os.getenv("ALERT_CHANNEL_ID"))
except (TypeError, ValueError):
    ALERT_CHANNEL_ID = 0
    logging.warning("ALERT_CHANNEL_ID not set or invalid.")

# --- å…¨å±€é…ç½® ---
MARKET_TIMEZONE = pytz.timezone('America/New_York')

SETTINGS_FILE = "/app/data/settings.json"
if not os.path.exists("/app/data"):
    SETTINGS_FILE = "settings.json"

# [é…ç½®] æ‰«ææ—¶é—´
TIME_MARKET_OPEN = time(9, 30)
TIME_MARKET_SCAN_START = time(10, 0) # 10ç‚¹æ‰å¼€å§‹æŠ¥
TIME_MARKET_CLOSE = time(16, 0)

# [é…ç½®] ç¾è‚¡ä¼‘å¸‚æ—¥ (2024-2025)
US_MARKET_HOLIDAYS = {
    # 2024
    "2024-01-01", "2024-01-15", "2024-02-19", "2024-03-29", "2024-05-27", "2024-06-19", 
    "2024-07-04", "2024-09-02", "2024-11-28", "2024-12-25",
    # 2025
    "2025-01-01", "2025-01-20", "2025-02-17", "2025-04-18", "2025-05-26", "2025-06-19",
    "2025-07-04", "2025-09-01", "2025-11-27", "2025-12-25"
}

# --- æ ¸å¿ƒç­–ç•¥é…ç½® (RVOL åŠ å¼ºç‰ˆ + å››ç»´å…±æŒ¯ + åŠ¨æ€å¸ƒæ—) ---
CONFIG = {
    # [1] è¿‡æ»¤å™¨ï¼šå·¦ä¾§æŠ„åº•æ ¸å¿ƒ (ä¸€ç¥¨å¦å†³åˆ¶)
    "filter": {
        "max_60d_gain": 0.3,          # [é˜²è¿½é«˜] è¿‡å»60å¤©æ¶¨å¹…è¶…è¿‡ 30% åˆ™ä¸çœ‹
        "max_rsi": 60,                # [é˜²è¿‡çƒ­] RSI(14) è¶…è¿‡ 60 åˆ™ä¸çœ‹
        "max_bias_50": 0.20,          # [é˜²å›è½] ç°ä»·åç¦» 50æ—¥å‡çº¿ 20% ä»¥ä¸Šä¸çœ‹
        "max_upper_shadow": 0.4,      # [é˜²æŠ›å‹] ä¸Šå½±çº¿é•¿åº¦å æ•´æ ¹Kçº¿ 40% ä»¥ä¸Šä¸çœ‹
        "max_day_change": 0.07,       # [é˜²å¦–è‚¡] å•æ—¥æ¶¨è·Œå¹…è¶…è¿‡ 7% ä¸çœ‹
        
        "min_rvol": 1.2,              # [æ ¸å¿ƒ] RVOL å¿…é¡» > 1.2 (æ¯”å†å²åŒæœŸæ´»è·ƒ20%ä»¥ä¸Š)
        
        # [å¸ƒæ—å¸¦åŠ¨æ€é…ç½® - ä¿®æ”¹éƒ¨åˆ†]
        "min_bb_squeeze_width": 0.10, # [å‰ç½®æ¡ä»¶] æ˜¨æ—¥å¸¦å®½éœ€å°äºæ­¤å€¼ (å®šä¹‰ä»€ä¹ˆæ˜¯"çª„")
        "bb_expansion_rate": 1.2,     # [åŠ¨æ€æ‰©å¼ ] ä»Šå¤©å¸¦å®½ / æ˜¨å¤©å¸¦å®½ >= 1.2 (å³æ‰©å¤§20%æ‰ç®—å¼€å£)
        "bb_squeeze_days": 10,        # æ–°å¢é…ç½®: ç›˜æ•´å¤©æ•°
        "bb_squeeze_tolerance": 0.05, # æ–°å¢é…ç½®: ç›˜æ•´å®¹å·®
        
        "max_bottom_pos": 0.30,       # [ä½ç½®] ä»·æ ¼åœ¨è¿‡å»60å¤©åŒºé—´çš„ä½ç½® (0.3è¡¨ç¤ºåº•éƒ¨30%)
        "min_adx_for_squeeze": 15     # [è¶‹åŠ¿] ADX æœ€å°é—¨æ§›ï¼Œç¡®ä¿ä¸æ˜¯æ­»æ°´
    },

    # [2] å½¢æ€è¯†åˆ«
    "pattern": {
        "pivot_window": 10,           # [å…³é”®ç‚¹] è¯†åˆ«é«˜ä½ç‚¹çš„å‰åçª—å£å¤©æ•°
        "support_tolerance": 0.02,    # æ–°å¢é…ç½®
        "support_window": 3           # æ–°å¢é…ç½®
    },

    # [3] ç³»ç»Ÿè®¾ç½®
    "system": {
        "cooldown_days": 3,           # [é˜²åˆ·å±] å‘å‡ºä¿¡å·åçš„å†·å´å¤©æ•°
        "max_charts_per_scan": 5,     # [é˜²æ‹¥å µ] æ¯æ¬¡æ‰«ææœ€å¤§å‘é€å›¾è¡¨æ•°é‡
        "history_days": 300           # [æ•°æ®æº] è·å–å†å²æ•°æ®çš„å¤©æ•° (ç”¨äºMA50/Ribbonç­‰)
    },

    # [4] æ‰“åˆ†ç³»ç»Ÿ
    "SCORE": { 
        "MIN_ALERT_SCORE": 70,        # [åŠæ ¼çº¿] æ€»åˆ†ä½äºæ­¤å€¼ä¸æŠ¥è­¦
        
        # [4.1] å››ç»´å…±æŒ¯è®¾ç½®
        "RESONANCE": {
            "window_days": 5,         # [çª—å£] å›æº¯è¿‡å» 5 å¤©å¯»æ‰¾èƒŒç¦»ä¿¡å·
            "min_signals": 2,         # [é˜ˆå€¼] è‡³å°‘éœ€è¦ 2 ä¸ªæŒ‡æ ‡åŒæ—¶èƒŒç¦»æ‰ç®—å…±æŒ¯
            "bonus_score": 30         # [åŠ åˆ†] è¾¾æˆå…±æŒ¯åçš„å¥–åŠ±åˆ†æ•°
        },

        # [4.2] ç­–ç•¥å‚æ•°
        "PARAMS": {
            "rvol_heavy": 2.0,              # [æœºæ„] RVOL > 2.0 è§†ä¸ºæœºæ„å¤§å•æ‰«è´§
            "rvol_capitulation": 2.5,       # [ææ…Œ] ææ…ŒæŠ›å”®æ—¶çš„é‡èƒ½è¦æ±‚
            
            "adx_strong_threshold": 25,     # [è¶‹åŠ¿] ADX > 25 è§†ä¸ºå¼ºè¶‹åŠ¿
            "adx_activation_lower": 20,     # [è¶‹åŠ¿] ADX < 20 è§†ä¸ºç›˜æ•´ï¼Œç”¨äºåˆ¤æ–­å¯åŠ¨
            "kdj_j_oversold": 0,            # [è¶…å–] KDJ.J < 0 è§†ä¸ºè¶…å–
            "divergence_price_tolerance": 1.02, # [èƒŒç¦»] ä»·æ ¼åˆ›æ–°ä½å®¹å·®
            "divergence_macd_strength": 0.8,    # [èƒŒç¦»] MACD æŸ±å­å¼ºåº¦çš„å®¹å·®
            "obv_lookback": 5,              # [èµ„é‡‘] OBV å›æº¯å¯¹æ¯”å¤©æ•°
            "capitulation_pinbar": 0.5      # [Kçº¿] é’ˆå‹Kçº¿åˆ¤æ–­é˜ˆå€¼
        },

        # [4.3] æƒé‡ (å„é¡¹å¾—åˆ†)
        "WEIGHTS": {
            # "4D_RESONANCE": 25,   # ç”± CONFIG["RESONANCE"]["bonus_score"] æ§åˆ¶
            
            "PATTERN_BREAK": 40,    # [å½¢æ€] æ——å½¢çªç ´ (æœ€é‡è¦)
            "PATTERN_SUPPORT": 20,  # [å½¢æ€] æ——å½¢æ”¯æ’‘å›è¸©
            "BB_SQUEEZE": 35,       # [å¸ƒæ—] æåº¦å‹ç¼©åçš„å¼€å£
            "STRONG_ADX": 20,       # [è¶‹åŠ¿] å¼ºè¶‹åŠ¿çŠ¶æ€
            "ADX_ACTIVATION": 25,   # [è¶‹åŠ¿] è¶‹åŠ¿ä»ç›˜æ•´ä¸­æ¿€æ´»
            "OBV_TREND_UP": 15,     # [èµ„é‡‘] OBV æŒç»­å‘ä¸Š
            
            "CAPITULATION": 25,     # [æŠ„åº•] ææ…Œç›˜æ¶Œå‡º (é…åˆ RVOL éªŒè¯)
            "HEAVY_INSTITUTIONAL": 20, # [é‡èƒ½] çº¯ç²¹çš„æœºæ„å¼‚åŠ¨ (é«˜ RVOL)
            
            "MACD_ZERO_CROSS": 10,  # [æŒ‡æ ‡] MACD 0è½´é‡‘å‰
            "MACD_DIVERGE": 10,     # [æŒ‡æ ‡] MACD åº•èƒŒç¦» (å¸¸è§„)
            "KDJ_REBOUND": 5,       # [æŒ‡æ ‡] KDJ è¶…å–åå¼¹
            "CANDLE_PATTERN": 5     # [Kçº¿] åæ²¡/æ™¨æ˜Ÿ/é”¤å­
        },

        "EMOJI": { 
            100: "TOP", 90: "HIGH", 80: "MID", 70: "LOW", 60: "TEST"
        }
    }
}
# --- é™æ€è‚¡ç¥¨æ±  ---
STOCK_POOLS = {
    "NASDAQ_100": [
        "AAPL", "MSFT", "AMZN", "NVDA", "META", "GOOGL", "GOOG", "TSLA", "AVGO", "ADBE", 
        "COST", "PEP", "CSCO", "NFLX", "AMD", "TMUS", "INTC", "CMCSA", "AZN", "QCOM", 
        "TXN", "AMGN", "HON", "INTU", "SBUX", "GILD", "BKNG", "DIOD", "MDLZ", "ISRG", 
        "REGN", "LRCX", "VRTX", "ADP", "ADI", "MELI", "KLAC", "PANW", "SNPS", "CDNS", 
        "CHTR", "MAR", "CSX", "ORLY", "MNST", "NXPI", "CTAS", "FTNT", "WDAY", "DXCM", 
        "PCAR", "KDP", "PAYX", "IDXX", "AEP", "LULU", "EXC", "BIIB", "ADSK", "XEL", 
        "ROST", "MCHP", "CPRT", "DLTR", "EA", "FAST", "CTSH", "VRSK", "CSGP", "ODFL", 
        "EBAY", "ILMN", "GFS", "ALGN", "TEAM", "CDW", "WBD", "SIRI", "ZM", "ENPH", 
        "JD", "PDD", "LCID", "RIVN", "ZS", "DDOG", "CRWD", "TTD", "BKR", "CEG", "GEHC", 
        "ON", "FANG"
    ],
    "GOD_TIER": [
        "NVDA", "AMD", "TSM", "SMCI", "AVGO", "ARM", "PLTR", "AI", "PATH", "BABA", 
        "PDD", "BIDU", "NIO", "LI", "XPEV", "COIN", "MARA", "MSTR"
    ]
}

settings = {}
rvol_baseline_cache = {} 

# --- è¾…åŠ©å‡½æ•° ---
def load_settings():
    global settings
    try:
        if os.path.exists(SETTINGS_FILE):
            with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
                settings = json.load(f)
        else:
            settings = {"users": {}, "signal_history": {}}
            save_settings()
    except Exception as e:
        logging.error(f"Error loading settings: {e}")
        settings = {"users": {}, "signal_history": {}}

def save_settings():
    try:
        dir_name = os.path.dirname(SETTINGS_FILE)
        if dir_name and not os.path.exists(dir_name):
            os.makedirs(dir_name, exist_ok=True)
        with open(SETTINGS_FILE, 'w', encoding='utf-8') as f:
            json.dump(settings, f, indent=4)
    except Exception as e:
        logging.error(f"Error saving settings: {e}")

def get_user_data(user_id):
    uid_str = str(user_id)
    if "users" not in settings: settings["users"] = {}
    if uid_str not in settings["users"]:
        settings["users"][uid_str] = {"stocks": [], "daily_status": {}}
    return settings["users"][uid_str]

# -----------------------------------------------------------------------------
# [æ ¸å¿ƒ] RVOL è®¡ç®—å™¨ - å·²ä¿®æ”¹ï¼šæ”¯æŒå¼€ç›˜å‰è·å–ï¼Œè¯†åˆ«å†¬å¤ä»¤æ—¶ï¼Œç¼©çŸ­å›æº¯æ—¶é—´
# -----------------------------------------------------------------------------
class RVOLCalculator:
    @staticmethod
    async def precalculate_baselines(symbols):
        global rvol_baseline_cache
        # è·å–ç¾ä¸œæ—¶é—´å½“å‰æ—¶é—´ï¼Œç”¨äºè®¡ç®—å‡†ç¡®çš„æ—¥æœŸèŒƒå›´
        ny_now = datetime.now(MARKET_TIMEZONE)
        logging.info(f"Start pre-calculating RVOL baselines for {len(symbols)} tickers. Time (ET): {ny_now.strftime('%Y-%m-%d %H:%M')}")
        
        # [ä¿®æ”¹] è®¡ç®—æ—¥æœŸèŒƒå›´ï¼šè¿‡å» 30 ä¸ªè‡ªç„¶æ—¥ (çº¦ 20-22 ä¸ªäº¤æ˜“æ—¥)
        # æ»¡è¶³"åªéœ€è¦20ä¸ªäº¤æ˜“æ—¥"çš„éœ€æ±‚ï¼ŒåŒæ—¶é¿å…å›æº¯ 40 å¤©è¿‡é•¿
        end_date_str = ny_now.strftime("%Y-%m-%d")
        start_date = ny_now - timedelta(days=30) 
        start_date_str = start_date.strftime("%Y-%m-%d")
        
        connector = aiohttp.TCPConnector(limit=5)
        semaphore = asyncio.Semaphore(5)
        
        async def fetch_intraday(session, sym):
            # è·å– 5åˆ†é’Ÿçº§åˆ« Kçº¿
            url = f"https://financialmodelingprep.com/stable/historical-chart/5min/{sym}?from={start_date_str}&to={end_date_str}&apikey={FMP_API_KEY}"
            async with semaphore:
                retries = 3
                for i in range(retries):
                    try:
                        async with session.get(url, ssl=False) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                return sym, data
                            elif resp.status == 429:
                                await asyncio.sleep(2)
                                continue
                    except: pass
                    await asyncio.sleep(0.5)
            return sym, []

        async with aiohttp.ClientSession(connector=connector) as session:
            tasks = [fetch_intraday(session, sym) for sym in symbols]
            results = await asyncio.gather(*tasks)

        count_ok = 0
        for sym, data in results:
            if not data: continue
            try:
                df = pd.DataFrame(data)
                if 'date' not in df.columns or 'volume' not in df.columns: continue
                
                # [å…³é”®] æ—¶é—´å¤„ç†ï¼šè¯†åˆ«å†¬å¤ä»¤æ—¶
                # FMP historical-chart é€šå¸¸è¿”å›çš„æ˜¯ç¾ä¸œæ—¶é—´çš„å¢™ä¸Šæ—¶é—´ï¼ˆWall Clock Timeï¼‰å­—ç¬¦ä¸²
                df['date'] = pd.to_datetime(df['date'])
                
                # å¦‚æœæ˜¯ naive time (æ²¡æœ‰æ—¶åŒºä¿¡æ¯)ï¼Œåˆ™å‡å®šä¸ºç¾ä¸œæ—¶é—´å¹¶æ·»åŠ æ—¶åŒºä¿¡æ¯
                if df['date'].dt.tz is None:
                    df['date'] = df['date'].dt.tz_localize(MARKET_TIMEZONE, ambiguous='NaT', nonexistent='shift_forward')
                else:
                    df['date'] = df['date'].dt.tz_convert(MARKET_TIMEZONE)
                
                # è¿‡æ»¤æ— æ•ˆæ—¶é—´ï¼ˆNaTï¼‰
                df = df.dropna(subset=['date'])

                df['time_str'] = df['date'].dt.strftime('%H:%M')
                df['date_only'] = df['date'].dt.date
                
                # ä»…ä¿ç•™äº¤æ˜“æ—¶æ®µ 09:30 - 16:00
                df = df[(df['time_str'] >= '09:30') & (df['time_str'] <= '16:00')]
                df = df.sort_values('date')
                
                # è®¡ç®—å½“æ—¥ç´¯è®¡æˆäº¤é‡
                df['cum_vol'] = df.groupby('date_only')['volume'].cumsum()
                
                # è®¡ç®—è¿‡å» N å¤©çš„ä¸­ä½æ•°ä½œä¸ºåŸºå‡†
                # è¿™é‡Œçš„æ•°æ®å·²ç»æ˜¯ç»è¿‡ç­›é€‰çš„è¿‡å» ~30 å¤©å†…çš„äº¤æ˜“æ—¥æ•°æ® (çº¦20ä¸ªäº¤æ˜“æ—¥)
                baseline = df.groupby('time_str')['cum_vol'].median()
                rvol_baseline_cache[sym] = baseline.to_dict()
                count_ok += 1
            except Exception as e:
                logging.error(f"Error processing RVOL for {sym}: {e}")
        logging.info(f"RVOL Baselines calculated for {count_ok} stocks (Range: {start_date_str} to {end_date_str}).")

    @staticmethod
    def get_current_rvol(ticker, current_cum_vol, ny_time):
        if ticker not in rvol_baseline_cache: return 1.0 
        minute = ny_time.minute
        floored_minute = (minute // 5) * 5
        time_key = f"{ny_time.hour:02d}:{floored_minute:02d}"
        baseline_vol = rvol_baseline_cache[ticker].get(time_key)
        if not baseline_vol or baseline_vol == 0: return 1.0
        return current_cum_vol / baseline_vol

# --- æ ¸å¿ƒé€»è¾‘ (æŒ‡æ ‡è®¡ç®—) ---
def calculate_indicators(df):
    cols = ['open', 'high', 'low', 'close', 'volume']
    for c in cols: df[c] = pd.to_numeric(df[c], errors='coerce')
    df = df[df['close'] > 0]
      
    price_col = 'close'
    exp12 = df[price_col].ewm(span=12, adjust=False).mean()
    exp26 = df[price_col].ewm(span=26, adjust=False).mean()
    df['DIF'] = exp12 - exp26
    df['DEA'] = df['DIF'].ewm(span=9, adjust=False).mean()
    df['MACD'] = (df['DIF'] - df['DEA']) * 2
      
    delta = df[price_col].diff()
    gain = (delta.clip(lower=0)).rolling(window=14).mean()
    loss = (-delta.clip(upper=0)).rolling(window=14).mean()
    rs = gain / loss.replace(0, 1e-9)
    df['RSI'] = 100 - (100 / (1 + rs))
    
    gain6 = (delta.clip(lower=0)).rolling(window=6).mean()
    loss6 = (-delta.clip(upper=0)).rolling(window=6).mean()
    rs6 = gain6 / loss6.replace(0, 1e-9)
    df['RSI6'] = 100 - (100 / (1 + rs6))
    
    gain12 = (delta.clip(lower=0)).rolling(window=12).mean()
    loss12 = (-delta.clip(upper=0)).rolling(window=12).mean()
    rs12 = gain12 / loss12.replace(0, 1e-9)
    df['RSI12'] = 100 - (100 / (1 + rs12))
      
    df['Vol_MA20'] = df['volume'].rolling(window=20).mean()
      
    df['tr1'] = df['high'] - df['low']
    df['tr2'] = abs(df['high'] - df['close'].shift(1))
    df['tr3'] = abs(df['low'] - df['close'].shift(1))
    df['TR'] = df[['tr1', 'tr2', 'tr3']].max(axis=1)
    df['ATR'] = df['TR'].rolling(window=14).mean()

    df['BB_Mid'] = df['close'].rolling(20).mean()
    df['BB_Std'] = df['close'].rolling(20).std()
    df['BB_Up'] = df['BB_Mid'] + 2 * df['BB_Std']
    df['BB_Low'] = df['BB_Mid'] - 2 * df['BB_Std']
    df['BB_Width'] = (df['BB_Up'] - df['BB_Low']) / df['BB_Mid']

    low_min = df['low'].rolling(9).min()
    high_max = df['high'].rolling(9).max()
    rsv_denom = (high_max - low_min).replace(0, 1e-9)
    df['RSV'] = (df['close'] - low_min) / rsv_denom * 100
    df['K'] = df['RSV'].ewm(com=2).mean() 
    df['D'] = df['K'].ewm(com=2).mean()
    df['J'] = 3 * df['K'] - 2 * df['D']

    alpha = 1/14
    df['up_move'] = df['high'] - df['high'].shift(1)
    df['down_move'] = df['low'].shift(1) - df['low']
    df['pdm'] = np.where((df['up_move'] > df['down_move']) & (df['up_move'] > 0), df['up_move'], 0)
    df['mdm'] = np.where((df['down_move'] > df['up_move']) & (df['down_move'] > 0), df['down_move'], 0)
    df['TR_s'] = df['TR'].ewm(alpha=alpha, adjust=False).mean()
    df['PDM_s'] = df['pdm'].ewm(alpha=alpha, adjust=False).mean()
    df['MDM_s'] = df['mdm'].ewm(alpha=alpha, adjust=False).mean()
    tr_s_safe = df['TR_s'].replace(0, 1e-9)
    df['PDI'] = 100 * (df['PDM_s'] / tr_s_safe)
    df['MDI'] = 100 * (df['MDM_s'] / tr_s_safe)
    dx_denom = (df['PDI'] + df['MDI']).replace(0, 1e-9)
    df['DX'] = 100 * abs(df['PDI'] - df['MDI']) / dx_denom
    df['ADX'] = df['DX'].ewm(alpha=alpha, adjust=False).mean()

    df['MA50'] = df['close'].rolling(50).mean()
    ma50_safe = df['MA50'].replace(0, np.nan) 
    df['BIAS_50'] = (df['close'] - ma50_safe) / ma50_safe

    candle_range = (df['high'] - df['low']).replace(0, 1e-9)
    upper_shadow = np.where(df['close'] >= df['open'], df['high'] - df['close'], df['high'] - df['open'])
    df['Upper_Shadow_Ratio'] = upper_shadow / candle_range

    obv_sign = np.sign(df['close'].diff()).fillna(0)
    df['OBV'] = (df['volume'] * obv_sign).cumsum()
    df['OBV_MA20'] = df['OBV'].rolling(window=20).mean()

    df['Ribbon_Fast'] = df['close'].ewm(span=21, adjust=False).mean()
    df['Ribbon_Slow'] = df['close'].ewm(span=60, adjust=False).mean()

    df['TP'] = (df['high'] + df['low'] + df['close']) / 3
    df['TP_MA'] = df['TP'].rolling(window=14).mean()
    df['TP_MAD'] = df['TP'].rolling(window=14).apply(lambda x: np.mean(np.abs(x - np.mean(x))), raw=True)
    df['CCI'] = (df['TP'] - df['TP_MA']) / (0.015 * df['TP_MAD'].replace(0, 1e-9))
    df['CCI_MA'] = df['CCI'].rolling(window=5).mean()

    df['RawMF'] = df['TP'] * df['volume']
    df['PosMF'] = np.where(df['TP'] > df['TP'].shift(1), df['RawMF'], 0)
    df['NegMF'] = np.where(df['TP'] < df['TP'].shift(1), df['RawMF'], 0)
    df['PosMF_Sum'] = df['PosMF'].rolling(window=14).sum()
    df['NegMF_Sum'] = df['NegMF'].rolling(window=14).sum()
    mf_total = df['PosMF_Sum'] + df['NegMF_Sum']
    df['MFI'] = 100 * (df['PosMF_Sum'] / mf_total.replace(0, 1e-9))
    df['MFI_MA'] = df['MFI'].rolling(window=6).mean()

    return df

def process_dataframe_sync(hist_data):
    if not hist_data: return None
    df = pd.DataFrame(hist_data)
    if 'date' not in df.columns: return None
    df['date'] = pd.to_datetime(df['date'])
    df = df.set_index('date').sort_index(ascending=True)
    return calculate_indicators(df)

def merge_and_recalc_sync(df, quote):
    if df is None or quote is None: return df
    try:
        quote_time = pd.to_datetime(quote['timestamp'], unit='s').tz_localize('UTC').tz_convert(MARKET_TIMEZONE)
        quote_date = quote_time.normalize().tz_localize(None) 
        last_idx = df.index[-1]
        last_date = pd.to_datetime(last_idx).normalize()
        if last_date.tzinfo is not None: last_date = last_date.tz_localize(None)

        current_price = quote['price']
        safe_high = max(quote['dayHigh'], current_price, quote['open'])
        safe_low = min(quote['dayLow'], current_price, quote['open'])

        new_row = {
            'open': quote['open'], 'high': safe_high, 'low': safe_low, 'close': current_price,
            'volume': quote['volume'], 'date': quote_date 
        }
        df_mod = df.copy()
        if last_date == quote_date:
            for col in ['open', 'high', 'low', 'close', 'volume']:
                if col == 'high': df_mod.at[last_idx, col] = max(df_mod.at[last_idx, col], new_row[col])
                elif col == 'low': df_mod.at[last_idx, col] = min(df_mod.at[last_idx, col], new_row[col])
                else: df_mod.at[last_idx, col] = new_row[col]
        elif last_date < quote_date:
            new_df = pd.DataFrame([new_row])
            new_df = new_df.set_index('date')
            df_mod = pd.concat([df_mod, new_df])
        if 'marketCap' in quote:
            df_mod.attrs['marketCap'] = quote['marketCap']
        return calculate_indicators(df_mod)
    except Exception as e:
        logging.error(f"[Merge Error] {e}")
        return df

def _safely_process_fmp_data(data, sym):
    try:
        if isinstance(data, list) and len(data) > 0 and 'date' in data[0] and 'close' in data[0]:
            return process_dataframe_sync(data)
        elif isinstance(data, dict) and 'historical' in data:
            return process_dataframe_sync(data['historical'])
        elif isinstance(data, list) and len(data) > 0 and 'historical' in data[0]:
            for item in data:
                if item.get('symbol') == sym:
                    return process_dataframe_sync(item['historical'])
        return None
    except Exception as e:
        logging.error(f"[Data Process Error] {sym}: {e}")
        return None

async def fetch_historical_batch(symbols: list, days=None):
    if not symbols: return {}
    if days is None: days = CONFIG["system"]["history_days"]
    results = {}
    now = datetime.now()
    from_date = (now - timedelta(days=days + 60)).strftime("%Y-%m-%d") 
    to_date = now.strftime("%Y-%m-%d")
    connector = aiohttp.TCPConnector(limit=3)
    semaphore = asyncio.Semaphore(3)
    headers = {"User-Agent": "Mozilla/5.0 (StockBot/1.0)", "Accept": "application/json"}

    async def fetch_single(session, sym):
        url = f"https://financialmodelingprep.com/stable/historical-price-eod/full?symbol={sym}&from={from_date}&to={to_date}&apikey={FMP_API_KEY}"
        async with semaphore:
            retries = 3
            for i in range(retries):
                try:
                    async with session.get(url, ssl=False) as response:
                        if response.status == 429:
                            wait_time = 4 * (2 ** i) 
                            logging.warning(f"[429 Rate Limit] {sym}. Retry {i+1}/{retries} in {wait_time}s...")
                            await asyncio.sleep(wait_time)
                            continue 
                        if response.status == 200:
                            data = await response.json()
                            df = await asyncio.to_thread(_safely_process_fmp_data, data, sym)
                            if df is not None and not df.empty: results[sym] = df
                            else: logging.warning(f"[æ•°æ®ä¸ºç©º] {sym}")
                            break 
                        else:
                            logging.error(f"[HTTP é”™è¯¯] {sym} Status: {response.status}")
                            break
                except Exception as e:
                    logging.error(f"[å¼‚å¸¸] {sym}: {e}")
                    break
            await asyncio.sleep(0.5)

    async with aiohttp.ClientSession(headers=headers, connector=connector) as session:
        tasks_list = [fetch_single(session, sym) for sym in symbols]
        await asyncio.gather(*tasks_list)
    return results

async def fetch_realtime_quotes(symbols: list):
    if not symbols: return {}
    quotes_map = {}
    connector = aiohttp.TCPConnector(limit=5)
    semaphore = asyncio.Semaphore(5)
    headers = {"User-Agent": "StockBot/1.0", "Accept": "application/json"}
    async def fetch_single_quote(session, sym):
        url = f"https://financialmodelingprep.com/stable/quote?symbol={sym}&apikey={FMP_API_KEY}"
        async with semaphore:
            retries = 3
            for i in range(retries):
                try:
                    async with session.get(url, ssl=False) as response:
                        if response.status == 429:
                            wait_time = 3 * (2 ** i) 
                            logging.warning(f"[429 Rate Limit] Quote {sym}. Retry {i+1}/{retries} in {wait_time}s...")
                            await asyncio.sleep(wait_time)
                            continue
                        if response.status == 200:
                            data = await response.json()
                            if isinstance(data, list):
                                for item in data:
                                    s = item.get('symbol')
                                    if s: quotes_map[s] = item
                            elif isinstance(data, dict):
                                     s = data.get('symbol')
                                     if s: quotes_map[s] = data
                            break
                        else:
                             logging.error(f"[Quote Error] {sym} Status: {response.status}")
                             break
                except Exception as e:
                    logging.error(f"[Quote Exception] {sym}: {e}")
                    break
            await asyncio.sleep(0.2)
    async with aiohttp.ClientSession(headers=headers, connector=connector) as session:
        tasks_list = [fetch_single_quote(session, sym) for sym in symbols]
        await asyncio.gather(*tasks_list)
    return quotes_map

async def fetch_market_index_data(days=60):
    now = datetime.now()
    from_date = (now - timedelta(days=days + 30)).strftime("%Y-%m-%d")
    to_date = now.strftime("%Y-%m-%d")
    url = f"https://financialmodelingprep.com/stable/historical-price-eod/light?symbol=%5EIXIC&from={from_date}&to={to_date}&apikey={FMP_API_KEY}"
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    if data and isinstance(data, list):
                        df = pd.DataFrame(data)
                        if 'date' in df.columns and 'price' in df.columns:
                            df['date'] = pd.to_datetime(df['date'])
                            df = df.set_index('date').sort_index(ascending=True)
                            return df
        except Exception as e:
            logging.error(f"[Market Index Error] {e}")
    return None

def find_pivots(df, window=10):
    highs = df['high'].values
    lows = df['low'].values
    dates = df.index
    pivots_high = [] 
    pivots_low = []
    lookback_days = 250
    start_idx = max(0, len(df) - lookback_days)
    for i in range(start_idx + window, len(df) - window):
        is_high = True
        is_low = True
        current_high = highs[i]
        current_low = lows[i]
        for j in range(1, window + 1):
            if highs[i-j] >= current_high or highs[i+j] > current_high: is_high = False
            if lows[i-j] <= current_low or lows[i+j] < current_low: is_low = False
        if is_high: pivots_high.append((dates[i], current_high, i))
        if is_low: pivots_low.append((dates[i], current_low, i))
    return pivots_high, pivots_low

def identify_patterns(df):
    if len(df) < 60: return None, [], [], None, None, None
    pivots_high, pivots_low = find_pivots(df, window=5)
    res_line, sup_line = [], []
    pattern_name = None
    min_anchor_idx = None
    vis_start_idx = max(0, len(df) - 250)
    curr_idx = len(df) - 1
    t_start = df.index[vis_start_idx]
    t_end = df.index[curr_idx]
    
    sup_slope = None
    sup_intercept = None
      
    if pivots_high:
        candidates_anchor = [p for p in pivots_high if p[2] < curr_idx - 15]
        if candidates_anchor:
            anchor = max(candidates_anchor, key=lambda x: x[1])
            idx_1, y1 = anchor[2], anchor[1]
            best_line = None
            max_slope = -float('inf') 
            for target in pivots_high:
                idx_2, y2 = target[2], target[1]
                if idx_2 <= idx_1 + 10: continue 
                m = (y2 - y1) / (idx_2 - idx_1)
                c = y1 - m * idx_1
                if m > 0: continue
                is_valid = True
                check_start = idx_1 + 1
                check_end = idx_2 - 1
                if check_end > check_start:
                    subset_highs = df['high'].iloc[check_start:check_end+1].values
                    subset_indices = np.arange(check_start, check_end+1)
                    line_vals = m * subset_indices + c
                    if np.any(subset_highs > line_vals * 1.02): is_valid = False
                if is_valid:
                    if m > max_slope:
                        max_slope = m
                        best_line = (m, c, idx_1, idx_2)
            if best_line:
                m, c, idx_1, idx_2 = best_line
                p_start = m * vis_start_idx + c
                p_end = m * curr_idx + c
                res_line = [[(t_start, p_start), (t_end, p_end)]]
                if min_anchor_idx is None or idx_1 < min_anchor_idx: min_anchor_idx = idx_1
                curr_price = df['close'].iloc[-1]
                if curr_price > p_end: pattern_name = "è¶‹åŠ¿çªç ´ (ç”±å®ˆè½¬æ”»)"

    if pivots_low:
        sorted_pivots = sorted(pivots_low, key=lambda x: x[1])
        potential_anchors = sorted_pivots[:5] 
        best_sup_line = None
        max_dist = 0 
        for anchor in potential_anchors:
            lx1, ly1 = anchor[2], anchor[1]
            targets = [p for p in pivots_low if p[2] > lx1 + 5]
            for target in targets:
                lx2, ly2 = target[2], target[1]
                m_sup = (ly2 - ly1) / (lx2 - lx1)
                c_sup = ly1 - m_sup * lx1
                if abs(m_sup) > 5: continue
                is_valid_sup = True
                check_start = lx1 + 1
                check_end = lx2 - 1
                if check_end > check_start:
                    subset_lows = df['low'].iloc[check_start:check_end+1].values
                    subset_indices = np.arange(check_start, check_end+1)
                    line_vals = m_sup * subset_indices + c_sup
                    if np.any(subset_lows < line_vals * 0.97): 
                        is_valid_sup = False
                if is_valid_sup:
                    dist = lx2 - lx1
                    if dist > max_dist:
                        max_dist = dist
                        best_sup_line = (m_sup, c_sup)
        if best_sup_line:
            m_sup, c_sup = best_sup_line
            sup_slope = m_sup
            sup_intercept = c_sup
            
            lp_start = m_sup * vis_start_idx + c_sup
            lp_end = m_sup * curr_idx + c_sup
            sup_line = [[(t_start, lp_start), (t_end, lp_end)]]
            if min_anchor_idx is None: min_anchor_idx = sorted_pivots[0][2] 

    return pattern_name, res_line, sup_line, min_anchor_idx, sup_slope, sup_intercept

def detect_candle_patterns(df):
    if len(df) < 5: return []
    patterns = []
    curr = df.iloc[-1]
    prev1 = df.iloc[-2]
    prev2 = df.iloc[-3]
    curr_body = abs(curr['close'] - curr['open'])
    prev1_body = abs(prev1['close'] - prev1['open'])
    prev2_body = abs(prev2['close'] - prev2['open'])
    is_bullish_engulfing = (prev1['close'] < prev1['open']) and \
                           (curr['close'] > curr['open']) and \
                           (curr['open'] < prev1['close']) and \
                           (curr['close'] > prev1['open'])
    if is_bullish_engulfing: patterns.append("Bullish Engulfing (åæ²¡)")
    is_morning_star = (prev2['close'] < prev2['open']) and \
                      (prev1_body < prev2_body * 0.3) and \
                      (curr['close'] > curr['open']) and \
                      (curr['close'] > (prev2['open'] + prev2['close'])/2)
    if is_morning_star: patterns.append("Morning Star (æ—©æ™¨ä¹‹æ˜Ÿ)")
    lower_shadow = min(curr['close'], curr['open']) - curr['low']
    upper_shadow = curr['high'] - max(curr['close'], curr['open'])
    if lower_shadow > 2 * curr_body and upper_shadow < 0.5 * curr_body: patterns.append("é”¤å­çº¿")
    return patterns

def calculate_risk_levels(df):
    curr_close = df['close'].iloc[-1]
    atr = df['ATR'].iloc[-1] if 'ATR' in df.columns else curr_close * 0.05
    stop_loss = curr_close - (2.8 * atr)
    _, pivots_low = find_pivots(df, window=5)
    support = stop_loss
    if pivots_low:
        last_pivot_low = pivots_low[-1][1]
        if last_pivot_low < curr_close: support = last_pivot_low
    return stop_loss, support

# -----------------------------------------------------------------------------
# [æ ¸å¿ƒä¿®æ”¹] æ•´åˆ RVOL çš„ä¿¡å·æ£€æŸ¥
# -----------------------------------------------------------------------------
def check_signals_sync(df, ticker):
    if len(df) < 60: return False, 0, "æ•°æ®ä¸è¶³", [], [], None, 1.0
    last_date = df.index[-1].date()
    today_date = datetime.now(MARKET_TIMEZONE).date()
    if (today_date - last_date).days > 4: return False, 0, f"DATA_STALE: æ•°æ®ä¸¥é‡æ»å ({last_date})", [], [], None, 1.0

    curr = df.iloc[-1]
    prev = df.iloc[-2]
    triggers = []
    score = 0
    weights = CONFIG["SCORE"]["WEIGHTS"]
    params = CONFIG["SCORE"]["PARAMS"]
    violations = [] 
    
    low_60 = df['low'].tail(60).min()
    high_60 = df['high'].tail(60).max()
    
    if curr['close'] > low_60 * (1 + CONFIG["filter"]["max_60d_gain"]): violations.append("è¿‡æ»¤å™¨: çŸ­æœŸæ¶¨å¹…è¿‡å¤§")
    
    prev_close_safe = prev['close'] if prev['close'] > 0 else 1.0
    day_gain = (curr['close'] - prev['close']) / prev_close_safe
    if abs(day_gain) > CONFIG["filter"]["max_day_change"]: violations.append("è¿‡æ»¤å™¨: å•æ—¥æ³¢åŠ¨è¿‡å¤§")
    
    if curr['RSI'] > CONFIG["filter"]["max_rsi"]: violations.append("è¿‡æ»¤å™¨: RSIä¸¥é‡è¶…ä¹°")
    
    if curr['BIAS_50'] > CONFIG["filter"]["max_bias_50"]: violations.append("è¿‡æ»¤å™¨: ä¹–ç¦»ç‡è¿‡å¤§")
    if curr['Upper_Shadow_Ratio'] > CONFIG["filter"]["max_upper_shadow"]: violations.append("è¿‡æ»¤å™¨: é•¿ä¸Šå½±çº¿å‹åŠ›")

    pattern_name, res_line, sup_line, anchor_idx, sup_slope, sup_intercept = identify_patterns(df)
    
    sup_tolerance = CONFIG["pattern"]["support_tolerance"]
    sup_window = CONFIG["pattern"]["support_window"]
    
    is_structure_support = False
    if sup_slope is not None:
        curr_idx = len(df) - 1
        curr_sup_price = sup_slope * curr_idx + sup_intercept
        if (1 - sup_tolerance/2) <= curr['close'] / curr_sup_price <= (1 + sup_tolerance):
            is_structure_support = True

    ny_now = datetime.now(MARKET_TIMEZONE)
    market_open = ny_now.replace(hour=9, minute=30, second=0, microsecond=0)
    minutes_elapsed = (ny_now - market_open).total_seconds() / 60
    is_open_market = 0 < minutes_elapsed < 390
    
    rvol = 1.0
    is_volume_ok = False
    
    if is_open_market and minutes_elapsed > 5:
        rvol = RVOLCalculator.get_current_rvol(ticker, curr['volume'], ny_now)
        if rvol >= CONFIG["filter"]["min_rvol"]:
            is_volume_ok = True
    else:
        is_volume_ok = True 
        
    if not is_volume_ok: 
        if is_structure_support:
            pass 
        else:
            violations.append(f"è¿‡æ»¤å™¨: èµ„é‡‘ä¸æ´»è·ƒ (RVOL {rvol:.2f} < 1.2)")
    
    if rvol > params["rvol_heavy"]:
        triggers.append(f"æœºæ„è¿›åœº: å¼‚å¸¸æ”¾é‡ (RVOL {rvol:.1f}x)")
        score += weights["HEAVY_INSTITUTIONAL"]

    candle_patterns = detect_candle_patterns(df)
    if candle_patterns:
        triggers.append(f"Kçº¿: {', '.join(candle_patterns)}")
        score += weights["CANDLE_PATTERN"]
    
    bb_squeeze_days = CONFIG["filter"]["bb_squeeze_days"]
    bb_squeeze_tol = CONFIG["filter"]["bb_squeeze_tolerance"]
    bb_expand_rate = CONFIG["filter"]["bb_expansion_rate"]
    max_cons_amp = CONFIG["filter"].get("max_consolidation_amp", 0.05)
    max_pos = CONFIG["filter"]["max_bottom_pos"]
    price_pos = (curr['close'] - low_60) / (high_60 - low_60) if high_60 > low_60 else 0.5
    
    if len(df) > bb_squeeze_days + 1:
        past_widths = df['BB_Width'].iloc[-(bb_squeeze_days+1):-1]
        past_closes = df['close'].iloc[-(bb_squeeze_days+1):-1]
        width_diff = past_widths.max() - past_widths.min()
        price_amp = (past_closes.max() - past_closes.min()) / past_closes.min()
        is_stable_width = width_diff <= bb_squeeze_tol
        is_sideways_price = price_amp <= max_cons_amp
        
        if is_stable_width and is_sideways_price:
            avg_width = past_widths.mean()
            width_ratio = curr['BB_Width'] / avg_width if avg_width > 0 else 1.0
            if width_ratio >= bb_expand_rate:
                if curr['close'] > curr['open']: 
                    if price_pos <= max_pos: 
                        triggers.append(f"BB Squeeze: ç›˜æ•´å¯åŠ¨ (ç¨³{bb_squeeze_days}æ—¥, æ‰©{width_ratio:.2f}x)")
                        score += weights["BB_SQUEEZE"]

    is_strong_trend = curr['ADX'] > params["adx_strong_threshold"] and curr['PDI'] > curr['MDI']
    is_adx_rising = curr['ADX'] > prev['ADX']
    if is_strong_trend and is_adx_rising: score += weights["STRONG_ADX"]
    
    recent_adx_min = df['ADX'].iloc[-10:-1].min()
    adx_activating = (recent_adx_min < params["adx_activation_lower"]) and \
                      (df['ADX'].iloc[-1] > df['ADX'].iloc[-2] > df['ADX'].iloc[-3])
    if adx_activating:
        triggers.append(f"è¶‹åŠ¿æ¿€æ´»: ç›˜æ•´ç»“æŸ ADXæ‹å¤´")
        score += weights["ADX_ACTIVATION"]

    pattern_scored = False 
    if pattern_name:
        triggers.append(pattern_name)
        score += weights["PATTERN_BREAK"]
        pattern_scored = True
    
    if not pattern_scored and sup_slope is not None:
        curr_idx = len(df) - 1
        def get_sup_price(idx): return sup_slope * idx + sup_intercept
        curr_sup = get_sup_price(curr_idx)
        is_on_support_now = (1 - sup_tolerance/2) <= curr['close'] / curr_sup <= (1 + sup_tolerance)
        
        if is_on_support_now:
            was_touching = False
            start_check_idx = max(0, curr_idx - sup_window)
            for i in range(start_check_idx, curr_idx):
                sup_at_i = get_sup_price(i)
                low_at_i = df['low'].iloc[i]
                if low_at_i <= sup_at_i * (1 + sup_tolerance):
                    was_touching = True
                    break
            if was_touching:
                triggers.append(f"æ——å½¢æ”¯æ’‘: è§¦åº•ä¼ç¨³ ({sup_window}æ—¥ç¡®è®¤)")
                score += weights["PATTERN_SUPPORT"]
                pattern_scored = True
            
            if not pattern_scored:
                was_broken = False
                start_check_idx = max(0, curr_idx - 6)
                for i in range(start_check_idx, curr_idx - 2): 
                    sup_at_i = get_sup_price(i)
                    if df['close'].iloc[i] < sup_at_i:
                        was_broken = True
                        break
                if was_broken:
                    triggers.append("æ——å½¢æ”¯æ’‘: å‡æ‘”å›è¸© (3æ—¥ç¡®è®¤)")
                    score += weights["PATTERN_SUPPORT"]
                    pattern_scored = True

    is_zero_cross = prev['DIF'] < 0 and curr['DIF'] > 0 and curr['DIF'] > curr['DEA']
    if is_zero_cross:
        triggers.append(f"MACD é‡‘å‰")
        score += weights["MACD_ZERO_CROSS"]
    
    if prev['J'] < params["kdj_j_oversold"] and curr['J'] > 0 and curr['K'] > curr['D']:
        triggers.append(f"KDJ åå‡»")
        score += weights["KDJ_REBOUND"]
    
    price_low_20 = df['close'].tail(20).min()
    price_is_low = curr['close'] <= price_low_20 * params["divergence_price_tolerance"]
    macd_low_20 = df['MACD'].tail(20).min()
    if price_is_low and curr['MACD'] < 0:
        if curr['MACD'] > macd_low_20 * params["divergence_macd_strength"] and curr['DIF'] > df['DIF'].tail(20).min():
             triggers.append(f"MACD åº•èƒŒç¦»")
             score += weights["MACD_DIVERGE"]
    
    if curr['OBV'] > curr['OBV_MA20']:
        obv_lookback = params["obv_lookback"]
        obv_rising = curr['OBV'] > df['OBV'].iloc[-obv_lookback]
        if obv_rising and curr['close'] > curr['open']:
             triggers.append("èµ„é‡‘é¢: OBVè¶‹åŠ¿å‘ä¸Š (èµ„é‡‘æµå…¥)")
             score += weights["OBV_TREND_UP"]

    if curr['low'] < curr['BB_Low']: 
        if rvol > params["rvol_capitulation"]:
            triggers.append(f"æŠ›å”®é«˜æ½®: ææ…Œç›˜æ¶Œå‡º (RVOL {rvol:.1f})")
            score += weights["CAPITULATION"]

    res_cfg = CONFIG["SCORE"]["RESONANCE"]
    res_window = res_cfg["window_days"]
    
    def check_divergence_window(series_val, series_sig, series_low, lookback):
        df_len = len(series_val)
        for i in range(df_len - lookback, df_len):
            if i <= 20: continue 
            if series_val[i-1] < series_sig[i-1] and series_val[i] > series_sig[i]:
                last_cross_idx = -1
                for j in range(i - 1, max(0, i - 60), -1):
                    if series_val[j-1] < series_sig[j-1] and series_val[j] > series_sig[j]:
                        last_cross_idx = j
                        break
                if last_cross_idx != -1:
                    price_lower = series_low[i] < series_low[last_cross_idx]
                    ind_higher = series_val[i] > series_val[last_cross_idx]
                    if price_lower and ind_higher:
                        return True
        return False

    s_low = df['low'].values
    div_macd = check_divergence_window(df['DIF'].values, df['DEA'].values, s_low, res_window)
    div_rsi = check_divergence_window(df['RSI6'].values, df['RSI12'].values, s_low, res_window)
    div_mfi = check_divergence_window(df['MFI'].values, df['MFI_MA'].values, s_low, res_window)
    div_cci = False
    cci_val = df['CCI'].values
    cci_ma = df['CCI_MA'].values
    for i in range(len(df) - res_window, len(df)):
        if i <= 20: continue
        if cci_val[i-1] < cci_ma[i-1] and cci_val[i] > cci_ma[i]:
            is_oversold = (cci_val[i] < -100) or (cci_val[i-1] < -100)
            if is_oversold:
                last_cross_idx = -1
                for j in range(i - 1, max(0, i - 60), -1):
                    if cci_val[j-1] < cci_ma[j-1] and cci_val[j] > cci_ma[j]:
                        last_cross_idx = j
                        break
                if last_cross_idx != -1:
                    if s_low[i] < s_low[last_cross_idx] and cci_val[i] > cci_val[last_cross_idx]:
                        div_cci = True
                        break

    resonance_count = sum([div_macd, div_rsi, div_mfi, div_cci])
    if resonance_count >= res_cfg["min_signals"]:
        triggers.append(f"å››ç»´å…±æŒ¯: {resonance_count}æŒ‡æ ‡åº•èƒŒç¦»")
        score += res_cfg["bonus_score"]

    is_triggered = (score >= CONFIG["SCORE"]["MIN_ALERT_SCORE"]) and (len(violations) == 0)
    final_reason_parts = triggers + violations
    final_reason = "\n".join(final_reason_parts) if final_reason_parts else "æ— æ˜æ˜¾ä¿¡å·"
    
    return is_triggered, score, final_reason, res_line, sup_line, anchor_idx, rvol

async def check_signals(df, ticker):
    return await asyncio.to_thread(check_signals_sync, df, ticker)

# -----------------------------------------------------------------------------
# å›¾è¡¨ç”Ÿæˆå‡½æ•°
# -----------------------------------------------------------------------------
def _generate_chart_sync(df, ticker, res_line=[], sup_line=[], stop_price=None, support_price=None, anchor_idx=None):
    buf = io.BytesIO()
      
    default_lookback = 100 
    start_idx = max(0, len(df) - default_lookback)
    if anchor_idx is not None: start_idx = max(0, anchor_idx - 30) 

    plot_df = df.iloc[start_idx:].copy()
    last_date = plot_df.index[-1]
    future_dates = pd.bdate_range(start=last_date + timedelta(days=1), periods=10) 
    future_df = pd.DataFrame(index=future_dates, columns=plot_df.columns)
    
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=FutureWarning)
        plot_df = pd.concat([plot_df, future_df])

    valid_df = plot_df.dropna(subset=['close', 'volume'])
    if not valid_df.empty:
        price_min = valid_df['low'].min()
        price_max = valid_df['high'].max()
        bins = np.linspace(price_min, price_max, 50)
        
        bull_df = valid_df[valid_df['close'] >= valid_df['open']]
        bear_df = valid_df[valid_df['close'] < valid_df['open']]
        
        vol_bull, _ = np.histogram(bull_df['close'], bins=bins, weights=bull_df['volume'])
        vol_bear, _ = np.histogram(bear_df['close'], bins=bins, weights=bear_df['volume'])
        
        bin_centers = (bins[:-1] + bins[1:]) / 2
        bar_height = (bins[1] - bins[0]) * 0.8
    else:
        vol_bull, vol_bear, bin_centers, bar_height = [], [], [], 0

    total_len = len(plot_df)                  
    if stop_price is None: stop_price = df['close'].iloc[-1] * 0.95
    if support_price is None: support_price = df['close'].iloc[-1] * 0.90
    stop_line_data = [stop_price] * total_len
    supp_line_data = [support_price] * total_len

    def clip_line_segments(segments):
        new_segments = []
        if not segments: return new_segments
        plot_start_date = plot_df.index[0]
        plot_start_ts = plot_start_date.timestamp()
        for seg in segments:
            d1, p1 = seg[0]
            d2, p2 = seg[1]
            try:
                if isinstance(d1, pd.Timestamp): d1_ts = d1.timestamp()
                else: d1_ts = d1.timestamp()
                if isinstance(d2, pd.Timestamp): d2_ts = d2.timestamp()
                else: d2_ts = d2.timestamp()
                if d2_ts < plot_start_ts: continue 
                if d1_ts < plot_start_ts:
                    if d2_ts - d1_ts == 0: continue
                    slope = (p2 - p1) / (d2_ts - d1_ts)
                    new_p1 = p1 + slope * (plot_start_ts - d1_ts)
                    new_segments.append([(plot_start_date, new_p1), (d2, p2)])
                else:
                    new_segments.append(seg)
            except: continue
        return new_segments

    res_line_clipped = clip_line_segments(res_line)
    sup_line_clipped = clip_line_segments(sup_line)

    premium_bg_color = '#131722'
    grid_color = '#2a2e39'
    text_color = '#b2b5be'
    volume_color = '#3b404e'
    
    my_marketcolors = mpf.make_marketcolors(
        up='#d93025',   
        down='#1db954', 
        edge='inherit',     
        wick='inherit',     
        volume=volume_color,
        ohlc='inherit'
    )
    
    my_style = mpf.make_mpf_style(
        base_mpl_style="dark_background",
        marketcolors=my_marketcolors,
        facecolor=premium_bg_color,
        figcolor=premium_bg_color,
        gridstyle=':', gridcolor=grid_color, gridaxis='both',
        rc={
            'font.family': 'sans-serif', 'axes.labelcolor': text_color,
            'xtick.labelcolor': text_color, 'ytick.labelcolor': text_color,
            'axes.edgecolor': grid_color,
            'ytick.left': False, 'ytick.right': True,
            'ytick.labelleft': False, 'ytick.labelright': True,
            'patch.linewidth': 0, 
        }
    )

    ribbon_fast = plot_df['Ribbon_Fast'].values
    ribbon_slow = plot_df['Ribbon_Slow'].values
    fb_bull = dict(y1=ribbon_fast, y2=ribbon_slow, where=ribbon_fast >= ribbon_slow, color='#00BFFF', alpha=0.1)
    fb_bear = dict(y1=ribbon_fast, y2=ribbon_slow, where=ribbon_fast < ribbon_slow, color='#FFFF00', alpha=0.1)

    add_plots = [
        mpf.make_addplot(plot_df['BB_Up'], color='#9370DB', linestyle=':', width=0.6, alpha=0.5),
        mpf.make_addplot(plot_df['BB_Mid'], color='#9370DB', linestyle=':', width=0.6, alpha=0.7), 
        mpf.make_addplot(plot_df['BB_Low'], color='#9370DB', linestyle=':', width=0.6, alpha=0.5),
        mpf.make_addplot(stop_line_data, color='red', linestyle='--', width=0.8, alpha=0.6), 
        mpf.make_addplot(supp_line_data, color='green', linestyle=':', width=0.8, alpha=0.6),
        mpf.make_addplot(plot_df['Ribbon_Fast'], width=0, alpha=0, fill_between=fb_bull),
        mpf.make_addplot(plot_df['Ribbon_Fast'], width=0, alpha=0, fill_between=fb_bear),
    ]
      
    seq_of_points = []
    if res_line_clipped:
        for line in res_line_clipped: seq_of_points.append([(line[0][0], float(line[0][1])), (line[1][0], float(line[1][1]))])
    if sup_line_clipped:
        for line in sup_line_clipped: seq_of_points.append([(line[0][0], float(line[0][1])), (line[1][0], float(line[1][1]))])

    kwargs = dict(
        type='candle', style=my_style, 
        ylabel='', addplot=add_plots, 
        volume=True, volume_panel=1, panel_ratios=(3, 1),
        tight_layout=True, datetime_format='%m-%d', xrotation=0, figsize=(10, 6),
        returnfig=True, 
        scale_padding={'right': 1.0}
    )
      
    if seq_of_points:
        kwargs['alines'] = dict(
            alines=seq_of_points,
            colors='#d1d4dc', linewidths=0.6, linestyle='-', alpha=0.4 
        )
      
    try:
        fig, axlist = mpf.plot(plot_df, **kwargs)
        ax_main = axlist[0]
        ax_main.text(0.5, 0.92, ticker, transform=ax_main.transAxes, fontsize=60, color='white', alpha=0.05, ha='center', va='top', weight='bold', zorder=0)

        if not valid_df.empty:
            ax_vp = ax_main.twiny()
            max_vol = max(vol_bull.max(), vol_bear.max()) if len(vol_bull) > 0 else 1
            ax_vp.set_xlim(0, max_vol * 4) 
            ax_vp.invert_xaxis() 
            ax_vp.barh(bin_centers, vol_bear, height=bar_height, color='#1db954', alpha=0.06, align='center', zorder=0)
            ax_vp.barh(bin_centers, vol_bull, height=bar_height, color='#d93025', alpha=0.06, align='center', left=vol_bear, zorder=0)
            ax_vp.axis('off')

        fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0.1, dpi=150)
        buf.seek(0)
    except Exception as e:
        logging.error(f"Plot Error: {e}")
        plt.close('all')
        return io.BytesIO()
    finally:
        plt.close('all')
        
    return buf

# [FIX] æ˜¾å¼å®šä¹‰ generate_chart å¼‚æ­¥åŒ…è£…å™¨
async def generate_chart(df, ticker, res_line=[], sup_line=[], stop_price=None, support_price=None, anchor_idx=None):
    return await asyncio.to_thread(_generate_chart_sync, df, ticker, res_line, sup_line, stop_price, support_price, anchor_idx)

# -----------------------------------------------------------------------------
# Embed ç”Ÿæˆå‡½æ•°
# -----------------------------------------------------------------------------
def create_alert_embed(ticker, score, price, reason, stop_loss, support, df, filename, rvol=None, is_filtered=False):
    title = f"ğŸš¨ {ticker} æŠ„åº•ä¿¡å· | å¾—åˆ† {score}"
    color = 0xe74c3c 

    embed = discord.Embed(
        title=title,
        description=f"**ç°ä»·:** `${price:.2f}`",
        color=color,
        timestamp=datetime.now(MARKET_TIMEZONE)
    )

    if not df.empty:
        curr = df.iloc[-1]
        rsi_val = f"{curr['RSI']:.1f}" if 'RSI' in df.columns else "N/A"
        adx_val = f"{curr['ADX']:.1f}" if 'ADX' in df.columns else "N/A"
        rvol_val = f"{rvol:.2f}x" if rvol is not None else "1.00x"
        bias_val = f"{curr['BIAS_50']*100:.1f}%" if 'BIAS_50' in df.columns else "N/A"
        obv_val = curr['OBV'] if 'OBV' in df.columns else 0
        obv_ma = curr['OBV_MA20'] if 'OBV_MA20' in df.columns else 0
        obv_status = "æµå…¥" if obv_val > obv_ma else "æµå‡º"

        left_col = (
            f"**RSI(14):** `{rsi_val}`\n"
            f"**ADX:** `{adx_val}`\n"
            f"**RVOL:** `{rvol_val}`\n"
            f"**OBV:** `{obv_status}`\n"
            f"**Bias(50):** `{bias_val}`"
        )
        right_col = (
            f"**æ­¢æŸä»·:** `${stop_loss:.2f}`\n"
            f"**æ”¯æ’‘ä½:** `${support:.2f}`"
        )
        embed.add_field(name="\u200b", value=left_col, inline=True)
        embed.add_field(name="\u200b", value=right_col, inline=True)

    if reason:
        embed.add_field(name="\u200b", value=f"```\n{reason}\n```", inline=False)
    else:
        embed.add_field(name="\u200b", value="```\næ— é¢å¤–è¯¦ç»†ä¿¡æ¯\n```", inline=False)

    embed.set_image(url=f"attachment://{filename}")
    embed.set_footer(text=f"StockBot Analysis | {ticker}")
    return embed

async def update_stats_data():
    pass

class StockBotClient(discord.Client):
    def __init__(self, *, intents: discord.Intents):
        super().__init__(intents=intents)
        self.tree = app_commands.CommandTree(self)
        self.alert_channel = None
        self.last_report_date = None
        self.last_baseline_prep_date = None # è®°å½•ä¸Šæ¬¡è¿è¡Œæ•°æ®å‡†å¤‡çš„æ—¥æœŸ

    async def on_ready(self):
        load_settings()
        logging.info(f'Logged in as {self.user}')
        if ALERT_CHANNEL_ID != 0:
            self.alert_channel = self.get_channel(ALERT_CHANNEL_ID)
            if self.alert_channel is None:
                logging.error(f"Could not find channel with ID {ALERT_CHANNEL_ID}")
        else:
            logging.warning("ALERT_CHANNEL_ID not set or invalid.")
        
        # å¯åŠ¨æ—¶å…ˆåˆå§‹åŒ–ä¸€æ¬¡
        asyncio.create_task(self.initialize_baselines())

        if not self.monitor_stocks.is_running():
            self.monitor_stocks.start()
        
        if not self.scheduled_report.is_running():
            self.scheduled_report.start()
            
        if not self.daily_market_prep.is_running():
            self.daily_market_prep.start()
            
        await self.tree.sync()

    async def initialize_baselines(self):
        users_data = settings.get("users", {})
        all_tickers = set()
        for u in users_data.values():
            all_tickers.update(u.get("stocks", []))
        for pool in STOCK_POOLS.values():
            all_tickers.update(pool)
        if all_tickers:
            await RVOLCalculator.precalculate_baselines(list(all_tickers))

    # [æ–°å¢] æ¯æ—¥å¼€ç›˜å‰å‡†å¤‡æ•°æ®çš„ä»»åŠ¡
    @tasks.loop(minutes=1)
    async def daily_market_prep(self):
        now_et = datetime.now(MARKET_TIMEZONE)
        today_date = now_et.date()
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºå‘¨æœ«
        if now_et.weekday() >= 5: return
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºç¾ä¸œæ—¶é—´ 09:15 (å¼€ç›˜å‰15åˆ†é’Ÿ) ä¸”ä»Šå¤©è¿˜æ²¡è¿è¡Œè¿‡
        if now_et.hour == 9 and now_et.minute == 15 and self.last_baseline_prep_date != today_date:
            logging.info(f"[{now_et.strftime('%H:%M')}] Pre-market Preparation Triggered.")
            await self.initialize_baselines()
            self.last_baseline_prep_date = today_date

    async def send_daily_stats_report(self):
        if not self.alert_channel: return
        logging.info("Generating daily backtest report...")
        await update_stats_data()
        load_settings()
        history = settings.get("signal_history", {})
        market_df = await fetch_market_index_data(days=80)

        def get_market_ret(date_str, offset_days):
            if market_df is None or market_df.empty: return None
            try:
                target_date = pd.to_datetime(date_str).normalize()
                idx = market_df.index.get_indexer([target_date], method='nearest')[0]
                if idx + offset_days < len(market_df):
                    p_start = market_df.iloc[idx]['price']
                    p_end = market_df.iloc[idx + offset_days]['price']
                    return ((p_end - p_start) / p_start) * 100
            except: pass
            return None

        stats_agg = {k: {"s_sum": 0.0, "s_c": 0, "m_sum": 0.0, "m_c": 0, "w": 0} for k in ["1d", "5d", "10d", "20d"]}
        
        valid_signals = []
        if history:
            sorted_dates = sorted(history.keys(), reverse=True)
            today = datetime.now().date()
            for date_str in sorted_dates:
                try:
                    sig_date = datetime.strptime(date_str, "%Y-%m-%d").date()
                except: continue
                if (today - sig_date).days > 25: continue 
                tickers_data = history[date_str]
                for ticker, data in tickers_data.items():
                    if data.get("score", 0) == 0: continue 
                    score = data.get("score", 0)
                    valid_signals.append((date_str, ticker, score, data))
                    for k, days_off in [("1d", 1), ("5d", 5), ("10d", 10), ("20d", 20)]:
                        m = get_market_ret(date_str, days_off)
                        if m is not None:
                            stats_agg[k]["m_sum"] += m
                            stats_agg[k]["m_c"] += 1
                        r = data.get(f"ret_{k}")
                        if r is not None:
                            stats_agg[k]["s_sum"] += r
                            stats_agg[k]["s_c"] += 1
                            if r > 0: stats_agg[k]["w"] += 1

        embed = discord.Embed(title="å›æµ‹ç»Ÿè®¡", color=0x9b59b6)
        def mk_field(key):
            d = stats_agg[key]
            if d["s_c"] > 0:
                avg_stock = d["s_sum"] / d["s_c"]
                avg_stock_str = f"`{avg_stock:+.2f}%`"
                win_rate = f"`{d['w']/d['s_c']*100:.0f}%`"
            else:
                avg_stock = None
                avg_stock_str = "Wait..."
                win_rate = "-"
            if d["m_c"] > 0:
                avg_market = d["m_sum"] / d["m_c"]
                avg_market_str = f"`{avg_market:+.2f}%`"
            else:
                avg_market = None
                avg_market_str = "Wait..."
            if avg_stock is not None and avg_market is not None and isinstance(avg_market, float):
                diff = avg_stock - avg_market
                diff_str = f"**{diff:+.2f}%**"
            else: diff_str = "-"
            return f"ä¸ªè‚¡å¹³å‡: {avg_stock_str}\nçº³æŒ‡åŒæœŸ: {avg_market_str}\nè¶…é¢æ”¶ç›Š: {diff_str}\nä¸ªè‚¡èƒœç‡: {win_rate}"

        embed.add_field(name="1æ—¥è¡¨ç°", value=mk_field("1d"), inline=True)
        embed.add_field(name="5æ—¥è¡¨ç°", value=mk_field("5d"), inline=True)
        embed.add_field(name="10æ—¥è¡¨ç°", value=mk_field("10d"), inline=True)
        embed.add_field(name="20æ—¥è¡¨ç°", value=mk_field("20d"), inline=True)
        
        recent_list_str = []
        for date_str, ticker, score, data in valid_signals[:5]:
            r1 = data.get("ret_1d")
            r1_str = f"{r1:+.1f}%" if r1 is not None else "-"
            r5 = data.get("ret_5d")
            r5_str = f"{r5:+.1f}%" if r5 is not None else "-"
            r10 = data.get("ret_10d")
            r10_str = f"{r10:+.1f}%" if r10 is not None else "-"
            r20 = data.get("ret_20d")
            r20_str = f"{r20:+.1f}%" if r20 is not None else "-"
            recent_list_str.append(f"`{date_str}` **{ticker}**\nâ”” 1D:`{r1_str}` 5D:`{r5_str}` 10D:`{r10_str}` 20D:`{r20_str}`")
        
        if recent_list_str: embed.add_field(name="è¯¦ç»†æƒ…å†µ", value="\n".join(recent_list_str), inline=False)
        else: embed.add_field(name="è¯¦ç»†æƒ…å†µ", value="æ— è¿‘æœŸä¿¡å·", inline=False)
        embed.set_footer(text=f"Report generated at {datetime.now(MARKET_TIMEZONE).strftime('%H:%M:%S')} ET")
        await self.alert_channel.send(embed=embed)

    @tasks.loop(minutes=1)
    async def scheduled_report(self):
        now_et = datetime.now(MARKET_TIMEZONE)
        if now_et.hour == 16 and now_et.minute == 30:
            today_date = now_et.date()
            if self.last_report_date != today_date:
                await self.send_daily_stats_report()
                self.last_report_date = today_date

    @tasks.loop(minutes=5)
    async def monitor_stocks(self):
        if not self.alert_channel: return
        now_et = datetime.now(MARKET_TIMEZONE)
        curr_time, today_str = now_et.time(), now_et.strftime('%Y-%m-%d')
        
        # [NEW] æ£€æŸ¥å‘¨æœ« (5=å‘¨å…­, 6=å‘¨æ—¥)
        if now_et.weekday() >= 5: 
            logging.info(f"[{today_str}] Weekend - Scan skipped.")
            return

        # [NEW] æ£€æŸ¥èŠ‚å‡æ—¥
        if today_str in US_MARKET_HOLIDAYS:
            logging.info(f"[{today_str}] Holiday - Scan skipped.")
            return

        is_open_scan = TIME_MARKET_SCAN_START <= curr_time <= TIME_MARKET_CLOSE
        if not is_open_scan: return
        
        logging.info(f"[{now_et.strftime('%H:%M')}] Scanning started...")
        users_data = settings.get("users", {})
        all_tickers = set()
        ticker_user_map = defaultdict(list)
        
        for uid, udata in users_data.items():
            for k in list(udata['daily_status'].keys()):
                if not k.endswith(today_str): del udata['daily_status'][k]
            for ticker in udata.get("stocks", []):
                all_tickers.add(ticker)
                ticker_user_map[ticker].append(uid)

        if not all_tickers: 
            logging.info("No tickers to scan.")
            return

        hist_map = await fetch_historical_batch(list(all_tickers))
        quotes_map = {}
        if TIME_MARKET_OPEN <= curr_time <= TIME_MARKET_CLOSE:
            quotes_map = await fetch_realtime_quotes(list(all_tickers))

        alerts_buffer = []
        if "signal_history" not in settings: settings["signal_history"] = {}
        if today_str not in settings["signal_history"]: settings["signal_history"][today_str] = {}

        for ticker, df_hist in hist_map.items():
            df = df_hist
            if ticker in quotes_map:
                df = await asyncio.to_thread(merge_and_recalc_sync, df_hist, quotes_map[ticker])
            if df is None or df.empty: continue

            user_ids = ticker_user_map[ticker]
            all_alerted = True
            users_to_ping = []
            for uid in user_ids:
                status_key = f"{ticker}-{today_str}"
                status = users_data[uid]['daily_status'].get(status_key, "NONE")
                if status == "NONE":
                    users_to_ping.append(uid)
                    all_alerted = False
            
            if all_alerted: continue

            history = settings.get("signal_history", {})
            in_cooldown = False
            cooldown_days = CONFIG["system"]["cooldown_days"]
            last_signal_score = 0
            
            for i in range(1, cooldown_days + 1): 
                past_date = (now_et.date() - timedelta(days=i)).strftime("%Y-%m-%d")
                if past_date in history and ticker in history[past_date]:
                    last_signal_score = history[past_date][ticker].get("score", 0)
                    in_cooldown = True 
            
            is_triggered, score, reason, res_line, sup_line, anchor_idx, rvol = await check_signals(df, ticker)
            
            today_signal_data = settings["signal_history"][today_str].get(ticker)
            if today_signal_data:
                today_score = today_signal_data.get("score", 0)
                if score <= today_score:
                    is_triggered = False
                    logging.info(f"Ticker {ticker} skipped because a signal with score {today_score} was already sent today.")

            if is_triggered and in_cooldown and last_signal_score > 0:
                if score <= last_signal_score:
                    logging.info(f"Ticker {ticker} skipped due to cooldown (Last Score: {last_signal_score}).")
                    is_triggered = False
            
            if is_triggered:
                price = df['close'].iloc[-1]
                stop_loss, support = calculate_risk_levels(df)
                alert_obj = {
                    "ticker": ticker,
                    "score": score, 
                    "priority": score, 
                    "price": price,
                    "reason": reason,
                    "support": support,
                    "stop_loss": stop_loss,
                    "df": df,
                    "res_line": res_line,
                    "sup_line": sup_line,
                    "anchor_idx": anchor_idx, 
                    "users": users_to_ping,
                    "rvol": rvol 
                }
                alerts_buffer.append(alert_obj)

        if alerts_buffer:
            alerts_buffer.sort(key=lambda x: x["priority"], reverse=True)
            max_charts = CONFIG["system"]["max_charts_per_scan"]
            sent_charts = 0
            summary_list = []

            for alert in alerts_buffer:
                ticker = alert["ticker"]
                score = alert["score"]
                users = alert["users"]
                
                current_hist = settings["signal_history"][today_str].get(ticker, {})
                settings["signal_history"][today_str][ticker] = {
                    "score": score,
                    "price": alert["price"],
                    "time": now_et.strftime('%H:%M'),
                    "reason": alert["reason"],
                    "ret_1d": current_hist.get("ret_1d"),
                    "ret_5d": current_hist.get("ret_5d"),
                    "ret_10d": current_hist.get("ret_10d"),
                    "ret_20d": current_hist.get("ret_20d"),
                }
                
                for uid in users:
                    status_key = f"{ticker}-{today_str}"
                    users_data[uid]['daily_status'][status_key] = "MARKET_SENT"
                
                mentions = " ".join([f"<@{uid}>" for uid in users])
                
                if sent_charts < max_charts:
                    chart_buf = await generate_chart(
                        alert["df"], ticker, alert["res_line"], alert["sup_line"], 
                        alert["stop_loss"], alert["support"], alert["anchor_idx"]
                    )
                    filename = f"{ticker}.png"
                    embed = create_alert_embed(
                        ticker, score, alert['price'], alert['reason'], 
                        alert['stop_loss'], alert['support'], alert['df'], filename,
                        rvol=alert["rvol"]
                    )
                    try:
                        file = discord.File(chart_buf, filename=filename)
                        await self.alert_channel.send(content=mentions, embed=embed, file=file)
                        sent_charts += 1
                        await asyncio.sleep(1.5)
                    except Exception as e: logging.error(f"Send Error: {e}")
                    finally: chart_buf.close() 
                else:
                    summary_list.append(f"**{ticker}** ({score})")

            if summary_list:
                summary_msg = f"**å…¶ä»–æé†’ (æ‘˜è¦)**:\n" + " | ".join(summary_list)
                try: await self.alert_channel.send(content=summary_msg)
                except: pass
            
            save_settings()
        logging.info(f"[{now_et.strftime('%H:%M')}] Scan finished. Alerts: {len(alerts_buffer)}")

intents = discord.Intents.default()
client = StockBotClient(intents=intents)

@client.tree.command(name="reset_stats", description="Reset all backtest statistics")
async def reset_stats(interaction: discord.Interaction):
    global settings
    settings["signal_history"] = {}
    save_settings()
    await interaction.response.send_message("Statistics reset.", ephemeral=True)

@client.tree.command(name="watch_add", description="Add stocks to watch list (e.g., AAPL, TSLA)")
@app_commands.describe(codes="Stock Symbols")
async def watch_add(interaction: discord.Interaction, codes: str):
    await interaction.response.defer()
    user_data = get_user_data(interaction.user.id)
    new_list = list(set([t.strip().upper() for t in codes.replace(',', ' ').replace('ï¼Œ', ' ').split() if t.strip()]))
    current_set = set(user_data["stocks"])
    current_set.update(new_list)
    user_data["stocks"] = list(current_set)
    save_settings()
    asyncio.create_task(RVOLCalculator.precalculate_baselines(new_list))
    await interaction.followup.send(f"Added: `{', '.join(new_list)}`")

@client.tree.command(name="watch_remove", description="Remove stocks from watch list")
@app_commands.describe(codes="Stock Symbols")
async def watch_remove(interaction: discord.Interaction, codes: str):
    await interaction.response.defer()
    user_data = get_user_data(interaction.user.id)
    to_remove = set([t.strip().upper() for t in codes.replace(',', ' ').replace('ï¼Œ', ' ').split() if t.strip()])
    current_list = user_data["stocks"]
    new_list = [s for s in current_list if s not in to_remove]
    user_data["stocks"] = new_list
    for t in to_remove:
        keys_to_del = [k for k in user_data['daily_status'] if k.startswith(t)]
        for k in keys_to_del: del user_data['daily_status'][k]
    save_settings()
    await interaction.followup.send(f"Removed: `{', '.join(to_remove)}`")

@client.tree.command(name="watch_list", description="Show my watch list")
async def watch_list(interaction: discord.Interaction):
    stocks = get_user_data(interaction.user.id)["stocks"]
    if len(stocks) > 60: display_str = ", ".join(stocks[:60]) + f"... ({len(stocks)})"
    else: display_str = ", ".join(stocks) if stocks else 'Empty'
    await interaction.response.send_message(f"List:\n`{display_str}`", ephemeral=True)

@client.tree.command(name="watch_clear", description="Clear all watched stocks")
async def watch_clear(interaction: discord.Interaction):
    user_data = get_user_data(interaction.user.id)
    user_data["stocks"] = []
    user_data["daily_status"] = {}
    save_settings()
    await interaction.response.send_message("Cleared.", ephemeral=True)

@client.tree.command(name="watch_import", description="Import preset lists")
@app_commands.choices(preset=[
    app_commands.Choice(name="NASDAQ 100", value="NASDAQ_100"),
    app_commands.Choice(name="GOD TIER", value="GOD_TIER")
])
async def watch_import(interaction: discord.Interaction, preset: app_commands.Choice[str]):
    await interaction.response.defer()
    user_data = get_user_data(interaction.user.id)
    new_list = STOCK_POOLS.get(preset.value, [])
    current_set = set(user_data["stocks"])
    current_set.update(new_list)
    user_data["stocks"] = list(current_set)
    save_settings()
    asyncio.create_task(RVOLCalculator.precalculate_baselines(new_list))
    await interaction.followup.send(f"Imported {preset.name} ({len(new_list)} stocks).")

@client.tree.command(name="stats", description="View historical signal accuracy (20-day window)")
async def stats_command(interaction: discord.Interaction):
    await interaction.response.defer()
    await update_stats_data()
    load_settings()
    history = settings.get("signal_history", {})
    market_df = await fetch_market_index_data(days=80)

    def get_market_ret(date_str, offset_days):
        if market_df is None or market_df.empty: return None
        try:
            target_date = pd.to_datetime(date_str).normalize()
            idx = market_df.index.get_indexer([target_date], method='nearest')[0]
            if idx + offset_days < len(market_df):
                p_start = market_df.iloc[idx]['price']
                p_end = market_df.iloc[idx + offset_days]['price']
                return ((p_end - p_start) / p_start) * 100
        except: pass
        return None

    stats_agg = {k: {"s_sum": 0.0, "s_c": 0, "m_sum": 0.0, "m_c": 0, "w": 0} for k in ["1d", "5d", "10d", "20d"]}
    seen_tickers = set()
    valid_signals = []
    
    sorted_dates = sorted(history.keys(), reverse=True)
    today = datetime.now().date()
    
    for date_str in sorted_dates:
        try: sig_date = datetime.strptime(date_str, "%Y-%m-%d").date()
        except: continue
        days_diff = (today - sig_date).days
        if days_diff > 25: continue
        tickers_data = history[date_str]
        for ticker, data in tickers_data.items():
            if data.get("score", 0) == 0: continue
            if ticker in seen_tickers: continue
            seen_tickers.add(ticker)
            score = data.get("score", 0)
            valid_signals.append((date_str, ticker, score, data))
            for k, days_off in [("1d", 1), ("5d", 5), ("10d", 10), ("20d", 20)]:
                m = get_market_ret(date_str, days_off) 
                if m is not None:
                    stats_agg[k]["m_sum"] += m
                    stats_agg[k]["m_c"] += 1
                r = data.get(f"ret_{k}")
                if r is not None:
                    stats_agg[k]["s_sum"] += r
                    stats_agg[k]["s_c"] += 1
                    if r > 0: stats_agg[k]["w"] += 1

    embed = discord.Embed(title="å›æµ‹ç»Ÿè®¡", color=0x00BFFF)
    def mk_field(key):
        d = stats_agg[key]
        if d["s_c"] > 0:
            avg_stock = d["s_sum"] / d["s_c"]
            avg_stock_str = f"`{avg_stock:+.2f}%`"
            win_rate = f"`{d['w']/d['s_c']*100:.0f}%`"
        else:
            avg_stock = None
            avg_stock_str = "Wait..."
            win_rate = "-"
        if d["m_c"] > 0:
            avg_market = d["m_sum"] / d["m_c"]
            avg_market_str = f"`{avg_market:+.2f}%`"
        else: avg_market_str = "Wait..."
        if avg_stock is not None and d["m_c"] > 0 and d["m_sum"] != 0:
            diff = avg_stock - (d["m_sum"] / d["m_c"])
            diff_str = f"**{diff:+.2f}%**"
        else: diff_str = "-"
        return f"ä¸ªè‚¡å¹³å‡: {avg_stock_str}\nçº³æŒ‡åŒæœŸ: {avg_market_str}\nè¶…é¢æ”¶ç›Š: {diff_str}\nä¸ªè‚¡èƒœç‡: {win_rate}"

    embed.add_field(name="1æ—¥è¡¨ç°", value=mk_field("1d"), inline=True)
    embed.add_field(name="5æ—¥è¡¨ç°", value=mk_field("5d"), inline=True)
    embed.add_field(name="10æ—¥è¡¨ç°", value=mk_field("10d"), inline=True)
    embed.add_field(name="20æ—¥è¡¨ç°", value=mk_field("20d"), inline=True)

    recent_list_str = []
    for date_str, ticker, score, data in valid_signals[:10]:
        r1 = data.get("ret_1d")
        r1_str = f"{r1:+.1f}%" if r1 is not None else "-"
        r5 = data.get("ret_5d")
        r5_str = f"{r5:+.1f}%" if r5 is not None else "-"
        r10 = data.get("ret_10d")
        r10_str = f"{r10:+.1f}%" if r10 is not None else "-"
        r20 = data.get("ret_20d")
        r20_str = f"{r20:+.1f}%" if r20 is not None else "-"
        recent_list_str.append(f"`{date_str}` **{ticker}**\nâ”” 1D:`{r1_str}` 5D:`{r5_str}` 10D:`{r10_str}` 20D:`{r20_str}`")
        
    if recent_list_str: embed.add_field(name="è¯¦ç»†æƒ…å†µ", value="\n".join(recent_list_str), inline=False)
    else: embed.add_field(name="è¯¦ç»†æƒ…å†µ", value="æ— è¿‘æœŸä¿¡å·", inline=False)
    await interaction.followup.send(embed=embed)

@client.tree.command(name="test", description="Test single stock")
async def test_command(interaction: discord.Interaction, ticker: str):
    await interaction.response.defer()
    ticker = ticker.upper().strip()
    logging.info(f"[TEST Command] Testing: {ticker}")

    # è¿™é‡Œè°ƒç”¨ precalculate_baselines æ—¶ï¼Œç°åœ¨åªä¼šå›æº¯ 30 å¤© (çº¦ 20 ä¸ªäº¤æ˜“æ—¥)ï¼Œé€Ÿåº¦ä¼šå˜å¿«
    await RVOLCalculator.precalculate_baselines([ticker])
    data_map = await fetch_historical_batch([ticker])
    quotes_map = await fetch_realtime_quotes([ticker])
    
    if not data_map or ticker not in data_map:
        await interaction.followup.send(f"Failed `{ticker}` (Check logs for 403/429 or data error)")
        return
        
    df = data_map[ticker]
    if ticker in quotes_map:
        df = await asyncio.to_thread(merge_and_recalc_sync, df, quotes_map[ticker])

    is_triggered, score, reason, r_l, s_l, anchor_idx, rvol = await check_signals(df, ticker)
    price = df['close'].iloc[-1]
    stop_loss, support = calculate_risk_levels(df)

    if not reason: reason = f"æ— æ˜æ˜¾ä¿¡å· (å¾—åˆ†: {score})"
    
    chart_buf = await generate_chart(df, ticker, r_l, s_l, stop_loss, support, anchor_idx)
    filename = f"{ticker}_test.png"
    is_filtered = score < CONFIG["SCORE"]["MIN_ALERT_SCORE"]
    
    embed = create_alert_embed(
        ticker, score, price, reason, stop_loss, support, df, filename, 
        rvol=rvol, is_filtered=is_filtered
    )

    try:
        f = discord.File(chart_buf, filename=filename)
        await interaction.followup.send(embed=embed, file=f)
    except Exception as e:
        logging.error(f"Send Error: {e}")
        await interaction.followup.send(f"Failed to send image: {e}")
    finally:
        chart_buf.close()

if __name__ == "__main__":
    if DISCORD_TOKEN: client.run(DISCORD_TOKEN)
